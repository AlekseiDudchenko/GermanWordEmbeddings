<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="GermanWordEmbeddings : Toolkit to obtain and preprocess german corpora, train models and evaluate them with generated testsets">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
    
    <title>GermanWordEmbeddings</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/devmount/GermanWordEmbeddings">View on GitHub</a>

          <h1 id="project_title">GermanWordEmbeddings</h1>
          <h2 id="project_tagline">A toolkit to obtain and preprocess German corpora, train models and evaluate them with generated testsets</h2>

            <section id="downloads">
              <a class="zip_download_link" target="_blank" href="https://github.com/devmount/GermanWordEmbeddings/zipball/master">Download the whole project as a .zip file</a>
              <a class="tar_download_link" target="_blank" href="https://github.com/devmount/GermanWordEmbeddings/tarball/master">Download the whole project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h2><a id="welcome" class="anchor" href="#welcome" aria-hidden="true"><span class="octicon octicon-link"></span></a>Welcome</h2>

<p>In my bachelor thesis I trained German word embeddings with <a href="http://radimrehurek.com/gensim/models/word2vec.html" target="_blank">gensim's word2vec library</a> and evaluated them with generated test sets. This page offers an overview about the project and <a href="#downloads" title="Jump to downloads">download links</a> for scripts, source and evaluation files. The whole project is licensed under <a href="https://github.com/devmount/GermanWordEmbeddings/blob/master/MIT.md">MIT license</a>.</p>

<h2><a id="training" class="anchor" href="#training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training and Evaluation</h2>
I found the following parameter configuration to be optimal to train german language models with word2vec:

<ul>
  <li>a corpus as big as possible (and as diverse as possible without being informal)</li>
  <li>filtering of punctuation and stopwords</li>
  <li>forming bigramm tokens</li>
  <li>using skip-gram as training algorithm with hierarchical softmax</li>
  <li>window size between 5 and 10</li>
  <li>dimensionality of feature vectors of 300 or more</li>
  <li>using negative sampling with 10 samples</li>
  <li>ignoring all words with total frequency lower than 50</li>
</ul>

<p>The following table shows some training stats for training a model with the above specification:</p>

<table>
  <tr>
    <td>training time</td>
    <td>6,16 h</td>
  </tr>
  <tr>
    <td>training speed</td>
    <td>26626 words/s</td>
  </tr>
  <tr>
    <td>vocab size</td>
    <td>608.130 words</td>
  </tr>
  <tr>
    <td>corpus size</td>
    <td>651.219.519 words</td>
  </tr>
  <tr>
    <td>model size</td>
    <td>720 MB</td>
  </tr>
</table>

<p>To train this model, you can take the following snippets after downloading this toolkit and navigating to its directory, where the <a target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/preprocessing.py">preprocessing.py</a> and the <a target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/training.py">training.py</a> script are used.</p>

<p>Make working directories:</p>
<p><pre><code>mkdir corpus
mkdir model</code></pre></p>

<p>Build news corpus:</p>
<p><pre><code>wget http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2013.de.shuffled.gz
gzip -d news.2013.de.shuffled.gz
python preprocessing.py news.2013.de.shuffled corpus/news.2013.de.shuffled.corpus -psub
rm news.2013.de.shuffled.gz</code></pre></p>

<p>Build wikipedia corpus:</p>
<p><pre><code>wget http://download.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xml.bz2
wget http://medialab.di.unipi.it/Project/SemaWiki/Tools/WikiExtractor.py
python WikiExtractor.py -c -b 25M -o extracted dewiki-latest-pages-articles.xml.bz2
find extracted -name '*bz2' \! -exec bzip2 -k -c -d {} \; > dewiki.xml
printf "Number of articles: "
grep -o "&lt;doc" dewiki.xml | wc -w
sed -i 's/<[^>]*>//g' dewiki.xml
rm -rf extracted
python preprocessing.py dewiki.xml corpus/dewiki.corpus -psub
rm dewiki.xml</code></pre></p>

<p>Training:</p>
<p><pre><code>python training.py corpus/ model/my.model -s 300 -w 5 -n 10 -m 50</code></pre></p>

<p>Subsequently the <a target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/evaluation.py">evaluation.py</a> script can be used to evaluate the trained model:</p>
<p><pre><code>python evaluation.py model/my.model -u -t 10</code></pre></p>

<p>Further examples and code explanation can be found in the following ipython notebooks:<br />
  <li><a target="_blank" href="https://github.com/devmount/GermanWordEmbeddings/blob/master/code/preprocessing.ipynb">Preprocessing</a></li>
  <li><a target="_blank" href="https://github.com/devmount/GermanWordEmbeddings/blob/master/code/training.ipynb">Training</a></li>
  <li><a target="_blank" href="https://github.com/devmount/GermanWordEmbeddings/blob/master/code/evaluation.ipynb">Evaluation</a></li>
</p>

<h2><a id="analogies" class="anchor" href="#analogies" aria-hidden="true"><span class="octicon octicon-link"></span></a>Semantic arithmetic</h2>
<p>With basic vector arithmetic it's possible to show the meaning of words that are representable by the model. Therefore the vectors are added or subtracted and with the help of the <a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank">cosine similarity</a> the vector(s) that are nearest to the result can be found. In the following, some interesting examples are shown:</p>

<!-- <pre><code>Mensch - Vernunft = Schimpanse <span style="float: right;">(0,349)</span></code></pre>
(human - reason = chimpanzee) -->
<p><pre><code>Frau + Kind = Mutter <span style="float: right;">(0,831)</span>
Frau + Hochzeit = Ehefrau <span style="float: right;">(0,795)</span></code></pre></p>
<p>A common family relationship: a woman with a child added is a mother. In word2vec terms: adding the vector of <code>child</code> to the vector of <code>woman</code> results in a vector which is closest to <code>mother</code> with a comparatively high cosine similarity of 0,831. In the same way a <code>woman</code> with a <code>wedding</code> results in a <code>wife</code>.</p>

<p><pre><code>Obama - USA + Russland = Putin <span style="float: right;">(0,780)</span></code></pre></p>
<p>The model is able to find a leader to a given country. Here <code>Obama</code> without <code>USA</code> is the feature for a country leader. Adding this feature to <code>Russia</code> results in <code>Putin</code>. It's also successful for other countries.</p>

<p><pre><code>Verwaltungsgebaeude + Buecher = Bibliothek <span style="float: right;">(0,718)</span>
Verwaltungsgebaeude + Buergermeister = Rathaus <span style="float: right;">(0,746)</span>
Haus + Filme = Kino <span style="float: right;">(0,713)</span>
Haus + Filme + Popcorn = Kino <span style="float: right;">(0,721)</span></code></pre></p>
<p>The relationship between a building and its function is found correctly. Here an <code>administration building</code> with <code>books</code> is logically the <code>library</code> and an <code>administration building</code> with a <code>mayor</code> is the <code>city hall</code>. Moreover a <code>house</code> with <code>movies</code> results in a <code>cinema</code>. Note that when adding <code>popcorn</code> to the equation, the resulting vector gets a little closer to the vector of the word <code>cinema</code>.</p>

<p><pre><code>Becken + Wasser = Schwimmbecken <span style="float: right;">(0,790)</span>
Sand + Wasser = Schlamm <span style="float: right;">(0,792)</span>
Meer + Sand = Strand <span style="float: right;">(0,725)</span></code></pre></p>
<p>Some nice examples with water: <code>sand</code> and <code>water</code> result in <code>mud</code>, <code>sea</code> and <code>sand</code> result in <code>beach</code> and a <code>basin</code> with <code>water</code> is a <code>pool</code>.</p>

<p><pre><code>Planet + Wasser = Erde <span style="float: right;">(0,717)</span>
Planet - Wasser = Pluto <span style="float: right;">(0,385)</span></code></pre></p>
<p>The main feature of our planet is correctly represented by the model: a <code>planet</code> with <code>water</code> is the <code>earth</code>, while a <code>planet</code> without <code>water</code> is <code>Pluto</code>. That's not quite accurate, because Pluto is made of water ice to one third...</p>

<p><pre><code>Kerze + Feuerzeug = brennende_Kerze <span style="float: right;">(0,768)</span></code></pre></p>
<p>Here is quite a good example for a semantically correct guess of a bigram token: <code>candle</code> and <code>lighter</code> result in a <code>burning_candle</code>.</p>

<p>The examples shown above are the results of a quick manual search for useful vector equations in the model. There are more amazing semantic relations for sure.</p>

<h2><a id="pca" class="anchor" href="#pca" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualizing features with PCA</h2>

The <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank">Principal Component Analysis</a> is a method to reduce the number of dimensions of high-dimensional vectors, while keeping main features (= the principal components). Therefore the 300 dimensions of the vectors of my German language model were reduced to a two-dimensional representation and plotted with pythons <a href="http://matplotlib.org" target="_blank">matplotlib</a> for some word classes.

<figure>
  <img src="images/pca_capital.svg" width="75%" alt="PCA: Capital of a country" title="PCA: Capital of a country"/>
  <figcaption>Countries and capitals are grouped correctly. The connecting lines are approximately parallel (except the one for Sweden maybe...) and of the same length. So the model understands the concept of capitals and countries.</figcaption>
</figure>

<figure>
  <img src="images/pca_currency.svg" width="75%" alt="PCA: Currency of a country" title="PCA: Currency of a country"/>
  <figcaption>Countries and their currencies are also grouped correctly. As well as the capitals, the concept of currencies of countries is well understood by the model. Note: <code>british_pounds</code> is here more accurate then just <code>pounds</code> because of multiple meanings of the word. Same with <code>US-Dollar</code> and <code>Dollar</code>.</figcaption>
</figure>

<figure>
  <img src="images/pca_language.svg" width="75%" alt="PCA: Language of a country" title="PCA: Language of a country"/>
  <figcaption>Finally another great example of grouped features with languages of countries.</figcaption>
</figure>

<p>The plots above are created with the <a target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/visualize.py">visualize.py</a> script of this project. Some further examples and code explanation can be found in the <a target="_blank" href="https://github.com/devmount/GermanWordEmbeddings/blob/master/code/pca.ipynb">PCA ipython notebook</a>.</p>


<h2><a id="download" class="anchor" href="#download" aria-hidden="true"><span class="octicon octicon-link"></span></a>Download</h2>

<h4>Model</h4>
<p>The German language model, trained with word2vec on the German Wikipedia (15th May 2015) and German news articles (15th May 2015):<br /><a class="download_link" target="_blank" href="https://tubcloud.tu-berlin.de/public.php?service=files&t=dc4f9d207bcaf4d4fae99ab3fbb1af16">german.model</a> [704 MB]<br />

<h4>Syntactic Questions</h4>
<p>10k questions with German umlauts:<br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/data/syntactic.questions">syntactic.questions</a><br />
The same 10k questions with transformed German umlauts:<br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/data/syntactic.questions.nouml">syntactic.questions.nouml</a>
</p>
<p>Evaluation source files:<br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/src/adjectives.txt">adjectives.txt</a><br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/src/nouns.txt">nouns.txt</a><br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/src/verbs.txt">verbs.txt</a></p>

<h4>Semantic Questions</h4>
<p>300 opposite questions with German umlauts:<br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/data/semantic_op.questions">semantic_op.questions</a><br />
The same 300 opposite questions with transformed German umlauts:<br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/data/semantic_op.questions.nouml">semantic_op.questions.nouml</a>
</p>
<p>540 best match questions with German umlauts:<br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/data/semantic_bm.questions">semantic_bm.questions</a><br />
The same 540 best match questions with transformed German umlauts:<br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/data/semantic_bm.questions.nouml">semantic_bm.questions.nouml</a>
</p>
<p>110 doesn't fit questions with German umlauts:<br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/data/semantic_df.questions">semantic_df.questions</a><br />
The same 110 doesn't fit questions with transformed German umlauts:<br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/data/semantic_df.questions.nouml">semantic_df.questions.nouml</a>
</p>
<p>Evaluation source files:<br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/src/opposite.txt">opposite.txt</a><br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/src/bestmatch.txt">bestmatch.txt</a><br /><a class="download_link" target="_blank" href="https://raw.githubusercontent.com/devmount/GermanWordEmbeddings/master/src/doesntfit.txt">doesntfit.txt</a></p>

  </section>
</div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">GermanWordEmbeddings maintained by <a href="https://github.com/devmount">devmount</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
